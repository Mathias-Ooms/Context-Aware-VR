\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}

\title[Your Short Title]{Context-Aware-VR}
\author{Mathias Ooms}
\institute{University of Antwerp}
\date{september 2020}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

\section{Introduction}

\begin{frame}{context-extraction}


Context characteristics: 

\begin{itemize}
	\item Eye tracking
	\item Position tracking
\end{itemize}

\end{frame}


\begin{frame}{Why reinforcement learning}


Supervised learning 

\begin{itemize}
	\item needs data to increase performance
	\item can never become better than the quality of the data
\end{itemize}

\end{frame}


\begin{frame}{Why reinforcement learning}

Reinforcement learning 

\begin{itemize}
	\item provides a way to have an agent learn completely by itself (without requiring data) 
	\item agents can become better than the 'best' players
\end{itemize}

\end{frame}


\begin{frame}{Reinforcement learning}

Policy Network
\begin{itemize}
	\item transforms the input to the possible actions 
\end{itemize}

Policy Gradients
\begin{itemize}
	\item method to train a policy network
\end{itemize}

\end{frame}


\begin{frame}{Policy Gradients}

\begin{itemize}
	\item starts off from a completely random network
\end{itemize}

\begin{figure}
	\includegraphics[scale=0.5]{"RL".jpg}
	\caption{The principle of RL}
\end{figure}

\begin{itemize}
	\item calculate for each possible action a probability
	\item agent will choose an action and get feedback (reward)
	\item the goal of the agent is to receive as much reward as possible
\end{itemize}

\end{frame}



\begin{frame}{Policy Gradients}

How does it improve the Policy network? 
\begin{itemize}
	\item In the first generations it will miserably fail, until the agent gets lucky (received a good reward)
	\item this successful series of actions now need to be more preferred over other ones
	\item when receiving a good reward we multiply the probability with a positive gradient
	\item when receiving a bad reward we multiply the probability with the same negative gradient
\end{itemize}

\end{frame}


\begin{frame}{"Troubles in paradise"}

Unfortunately there are also downsides:
\begin{itemize}
	\item $Credit\ assignment\ problem$: punished hard for small mistakes
	\item Not appropriate in complex environments
\end{itemize}

\end{frame}



\end{document}
